# Practical ML Summer 2015

**Author**: Elizabeth Barayuga

##Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. 
<br>
These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
<br>
In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 
<br>
More information is available from the website here: ***http://groupware.les.inf.puc-rio.br/har***
(see the section on the Weight Lifting Exercise Dataset). 
<br>
<br>


##Data Involved in Model

The training data for this project are available here: 

**https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv**

The test data are available here: 

**https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv**

The data for this project come from this source: **http://groupware.les.inf.puc-rio.br/har**. 



## Goal

The goal of your project is to predict the manner in which they did the exercise. 
<br>
This is the **"classe"** variable in the training set. 

There are a total of 160 possible predictors and can be used to predict the **"classe"** variable.

## Approach for Developing the Model

1. Analyze the values for each predictors
2. Cleanse the data 
3. Select possible features that would be used for the prediction model 
4. Identify possible method that would be used for prediction
5. Train the model selected by using training. 
6. Identify a threshold for the training data that would be used for training 
7. Validate the model by running it to the testing set that was set-aside from the training data 
8. Check the confusion matrix for the performance of the model 
9. Once acceptable, run the final validation on the final testing file 


## Methodology for Cleansing Data 

	1. Represent all empty data as "NA"
	2. Replace all  DIV/0! data as "NA"
	
## Feature Selection 

In any model, it would be good to analyze the data to see what features can be used to predict the **classe** variable. 

The general goal of a good model is to be able to use the right features that can generalize well that it can be used to model with unseen data. 

1. Run str to get an understanding of the different features available in training data. 
	
		str(training) 
	
2. Furthermore, we removed any features that contain NA since it would not be useful for our model
		
		nacolumns <- rawtrain[,sapply(.SD, function(x) any(is.na(x)))]
		
3. Columns that appear to be more informative of the participant of the data were also removed. 
		
		rownames - V1
		user_name 
		raw_timestamp_part_1
		raw_timestamp_part_2
		cvtd_timestamp
		new_window
		num_window


<br>
		
		
## Cross Validation 

<br>

In order to train the model, we have selected divide the training file into a training and testing set . 
The threshold that was used is *60%*


	intrain  <- createDataPartition(rawtrainfeatures$classe, p=.60, list=FALSE)
	training <- rawtrainfeatures[intrain[,1]]
	testing  <- rawtrainfeatures[-intrain[,1]]
	

The threshold was changed to *75%* for further validation during the first round and it shows an increase in accuracy. In the end it was decided to use *60%* for the final model.

<br>	
	
## Model Prediction 

<br>

**FIRST TEST** 

We decided to use the **Random Forest Model to be used for prediction of the **classe** variable

	model.rf <- train(y=as.factor(training$classe), 
                x=training[,!"classe",with=F], 
                tuneGrid=data.frame(mtry=3), 
      			trControl=trainControl(method="none"), 
      			method="parRF") 

<br>
The following was the validated with the testing set.

	confusionMatrix(predict(model.rf,newdata=transformfeatures(testing)), factor(testing$classe)) 
<br>

The first test showed the following Confusion Matrix Results using *60%* threshold. 

	Confusion Matrix and Statistics

          					Reference
		Prediction    A    B    C    D    E
         			A 2227    9    0    0    0
         			B    4 1505   16    0    0
         			C    0    4 1349   30    0
         			D    1    0    2 1256    6
         			E    0    0    1    0 1436


	Overall Statistics
                                          
               Accuracy : 0.9907          
                 95% CI : (0.9883, 0.9927)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9882          
 	Mcnemar's Test P-Value : NA              


	Statistics by Class:

                     		Class: A Class: B Class: C Class: D Class: E
	Sensitivity            0.9978   0.9914   0.9861   0.9767   0.9958
	Specificity            0.9984   0.9968   0.9948   0.9986   0.9998
	Pos Pred Value         0.9960   0.9869   0.9754   0.9929   0.9993
	Neg Pred Value         0.9991   0.9979   0.9971   0.9954   0.9991
	Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
	Detection Rate         0.2838   0.1918   0.1719   0.1601   0.1830
	Detection Prevalence   0.2850   0.1944   0.1763   0.1612   0.1832
	Balanced Accuracy      0.9981   0.9941   0.9904   0.9876   0.9978

<br> 
**SECOND TEST** 

Using the same model but using 75% threshold, the accuracy increase from *.9907* to *.9953*

	intrain2  <- createDataPartition(rawtrainfeatures$classe, p=.75, list=FALSE)
	training2 <- rawtrainfeatures[intrain2[,1]]
	testing2  <- rawtrainfeatures[-intrain2[,1]]


	model2.rf <- train(y=as.factor(training2$classe), 
                 x=training2[,!"classe",with=F], 
                 tuneGrid=data.frame(mtry=3), 
                 trControl=trainControl(method="none"), method="parRF") 
                 
 
 <br>
 The following was the validated with the testing2 set.
                   
		confusionMatrix(predict(model2.rf,newdata=transformfeatures(testing2)), factor(testing2$classe))

<br> 
The secobd test showed the following Confusion Matrix Results using *75%* threshold. 

<br>

	Confusion Matrix and Statistics

          						Reference
			Prediction    A    B    C    D    E
         				A 1394    5    0    0    0
         				B    1  943    4    0    0
         				C    0    1  850    6    0
         				D    0    0    1  795    2
         				E    0    0    0    3  899

	Overall Statistics
                                        
               Accuracy : 0.9953        
                 95% CI : (0.993, 0.997)
    No Information Rate : 0.2845        
    P-Value [Acc > NIR] : < 2.2e-16     
                                        
                  Kappa : 0.9941        
 		Mcnemar's Test P-Value : NA            

	Statistics by Class:

                   			Class: A Class: B Class: C Class: D Class: E
	Sensitivity            0.9993   0.9937   0.9942   0.9888   0.9978
	Specificity            0.9986   0.9987   0.9983   0.9993   0.9993
	Pos Pred Value         0.9964   0.9947   0.9918   0.9962   0.9967
	Neg Pred Value         0.9997   0.9985   0.9988   0.9978   0.9995
	Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
	Detection Rate         0.2843   0.1923   0.1733   0.1621   0.1833
	Detection Prevalence   0.2853   0.1933   0.1748   0.1627   0.1839
	Balanced Accuracy      0.9989   0.9962   0.9962   0.9940   0.9985